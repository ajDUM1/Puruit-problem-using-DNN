{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wLRgc0NtmqS"
      },
      "source": [
        "Import Statements and Set Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRBw1FqrjXj_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.layers import Conv1D, Dense, Flatten, Activation, MaxPooling1D, Dropout\n",
        "from math import sin, cos, pi\n",
        "from numpy.random import randint\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "np.random.seed(295)\n",
        "random.seed(295)\n",
        "\n",
        "#CONSTANTS\n",
        "SIN5 = sin(5*pi/180)\n",
        "COS5 = cos(5*pi/180)\n",
        "\n",
        "# Environment settings\n",
        "EPISODES = 75_000\n",
        "\n",
        "#episodic batch size\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 0.96\n",
        "EPSILON_DECAY = 0.999\n",
        "MIN_EPSILON = 0.01\n",
        "\n",
        "#Discount factor\n",
        "DISCOUNT = 0.98\n",
        "\n",
        "#target update value\n",
        "COUNTER_MAX = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memory Class"
      ],
      "metadata": {
        "id": "FRwTnl7prJz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory:\n",
        "    \n",
        "    def __init__(self):\n",
        "\n",
        "        self.alog = np.array([0,0,0]) #log for action\n",
        "        self.rlog = [] #log for reward\n",
        "        self.slog = 0 #log for success\n",
        "        self.sslog = [] #log for success per step\n",
        "        self.clog = 0 #log for collision\n",
        "        self.cclog = [] #log for collisions per step\n",
        "        self.elog = 0 #log for number of episodes\n",
        "        self.llog = [] #log for the episodic loss\n",
        "\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        #environment data\n",
        "        self.p = np.array([])\n",
        "        self.t = np.array([])\n",
        "\n",
        "        self.px = np.array([])\n",
        "        self.py = np.array([])"
      ],
      "metadata": {
        "id": "pyWXYsd7rMCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t6nJI4o7ibq"
      },
      "source": [
        "Environment Class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchEnvironment:\n",
        "    SIZE = 600\n",
        "    \n",
        "    MOVE_REWARD = -1\n",
        "    CAPTURE_REWARD = 25\n",
        "    COLLISION_REWARD = -8\n",
        "\n",
        "    MAX_EPISODE_STEP = 300\n",
        "\n",
        "    def __init__(self, BATCH_SIZE):\n",
        "        self.BATCH_SIZE = BATCH_SIZE\n",
        "\n",
        "        self.rm = ReplayMemory()\n",
        "\n",
        "        self.episode_step = np.ones(self.BATCH_SIZE)*self.MAX_EPISODE_STEP\n",
        "    \n",
        "    def reset(self):\n",
        "        #clear the replay buffer\n",
        "        self.rm.reset()\n",
        "\n",
        "        #generate new random states\n",
        "        self.rm.p = randint(0,self.SIZE,(self.BATCH_SIZE,2))*1.0\n",
        "        self.rm.t = randint(0,self.SIZE,(self.BATCH_SIZE,2))*1.0\n",
        "\n",
        "        #genrating random theta for vx, vy\n",
        "        th = 2*np.pi*np.random.random(self.BATCH_SIZE)\n",
        "        self.rm.px = (0.9*self.SIZE/200)*np.cos(th)\n",
        "        self.rm.py = (0.9*self.SIZE/200)*np.sin(th)\n",
        "\n",
        "        #reset episode step\n",
        "        self.episode_step = np.ones(self.BATCH_SIZE)*self.MAX_EPISODE_STEP\n",
        "    \n",
        "    def getCS(self):\n",
        "        return np.concatenate(((self.rm.p-self.rm.t)/self.SIZE,self.rm.p/self.SIZE,self.rm.px[None].T/6,self.rm.py[None].T/6), axis=1)\n",
        "    \n",
        "    def step(self, action):\n",
        "        #create a current state for training\n",
        "        cs = self.getCS()\n",
        "\n",
        "        #update positions first\n",
        "        self.rm.p[:,0] += self.rm.px\n",
        "        self.rm.p[:,1] += self.rm.py\n",
        "\n",
        "        #update the velocity\n",
        "        temp = self.rm.px.copy()\n",
        "        self.rm.px = self.rm.px*COS5 - self.rm.py*action*SIN5\n",
        "        self.rm.py = self.rm.py*COS5 + temp*action*SIN5\n",
        "\n",
        "        #update the step counter\n",
        "        self.episode_step -= 1\n",
        "\n",
        "        #create a new state for training\n",
        "        ns = self.getCS()\n",
        "\n",
        "        #check for terminal states\n",
        "        capture = np.sum(np.abs(self.rm.p-self.rm.t), axis=1)<self.SIZE/20\n",
        "        self.rm.slog += capture.sum()\n",
        "        self.episode_step[capture] = 0\n",
        "\n",
        "        outabound = np.logical_or((self.rm.p>self.SIZE),(self.rm.p<0))\n",
        "        collision = np.logical_or(outabound[:,0],outabound[:,1])\n",
        "        self.rm.clog += collision.sum()\n",
        "\n",
        "        self.episode_step[collision] = 0\n",
        "\n",
        "        done = (self.episode_step==0)\n",
        "        s = done.sum()\n",
        "        self.rm.elog += s\n",
        "\n",
        "        if s:\n",
        "            #generate new states for terminal ones\n",
        "            self.rm.p[done] = randint(0,self.SIZE,(s,2))*1.0\n",
        "            self.rm.t[done] = randint(0,self.SIZE,(s,2))*1.0\n",
        "\n",
        "            th = 2*np.pi*np.random.random(s)\n",
        "            self.rm.px[done] = (0.9*self.SIZE/200)*np.cos(th)\n",
        "            self.rm.py[done] = (0.9*self.SIZE/200)*np.sin(th)\n",
        "\n",
        "            #reset episode_step\n",
        "            self.episode_step[done] = self.MAX_EPISODE_STEP\n",
        "\n",
        "        #calculate rewards\n",
        "        reward = np.ones(self.BATCH_SIZE)*self.MOVE_REWARD + capture*self.CAPTURE_REWARD + collision*self.COLLISION_REWARD\n",
        "\n",
        "        self.rm.rlog.append(reward.mean())\n",
        "\n",
        "        #return cs, ns, reward, done\n",
        "        return cs, ns, reward, done"
      ],
      "metadata": {
        "id": "2uwLi8Y0qtVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Q-Learning Network"
      ],
      "metadata": {
        "id": "bjENljDqyaDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN:\n",
        "    def __init__(self):\n",
        "\n",
        "        # Main model\n",
        "        self.model = self.create_model()\n",
        "\n",
        "        self.target_model = self.create_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        self.update_counter = 0\n",
        "\n",
        "    def create_model(self):\n",
        "        model = keras.Sequential()\n",
        "\n",
        "        model.add(keras.Input((6,1)))\n",
        "\n",
        "        model.add(Conv1D(128, 3, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
        "\n",
        "        model.add(Conv1D(128, 5, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
        "\n",
        "        model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "        model.add(Dense(32))\n",
        "\n",
        "        model.add(Dense(3, activation='linear'))  # ACTION_SPACE_SIZE = how many choices (9)\n",
        "        model.compile(loss=\"mse\", optimizer='adam', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def train(self, cs, qcs, ns, action, reward, done):\n",
        "        #query target value for future max rewards\n",
        "        qns = self.target_model.predict(ns)\n",
        "\n",
        "        #get the maximum future reward\n",
        "        max_qns = np.max(qns, axis=1)\n",
        "\n",
        "        #the discounted maximum future reward for non terminal state\n",
        "        new_qns = reward + DISCOUNT*max_qns*np.invert(done)\n",
        "\n",
        "        #create a mask from action space\n",
        "        mask = np.concatenate((action==-1,action==0,action==1)).reshape((3,action.size)).T\n",
        "\n",
        "        #set get the true predictions\n",
        "        fit_qns = np.copy(qcs)\n",
        "        fit_qns[mask] = new_qns\n",
        "\n",
        "        loss = (qcs-fit_qns).mean()\n",
        "        mask = np.random.choice(cs.shape[0],64,replace=False)\n",
        "\n",
        "        #fit the model\n",
        "        self.model.fit(cs[mask], fit_qns[mask], batch_size=64, verbose=0, shuffle=True)\n",
        "\n",
        "        #update the target network\n",
        "        if done.sum():\n",
        "            self.update_counter += 1\n",
        "            \n",
        "        if self.update_counter == COUNTER_MAX:\n",
        "            self.target_model.set_weights(self.model.get_weights())\n",
        "            self.update_counter=0\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    \n",
        "    #return the q value\n",
        "    def get_qs(self, state):\n",
        "        return self.model.predict(state)"
      ],
      "metadata": {
        "id": "cUBRMsdLyeic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "fYepNPRt_gMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = DQN()\n",
        "\n",
        "env = BatchEnvironment(BATCH_SIZE)\n",
        "env.reset()\n",
        "\n",
        "#network.model = tf.keras.models.load_model('/content/drive/MyDrive/model2_alt')\n",
        "#network.target_model.set_weights(network.model.get_weights())\n",
        " \n",
        "\n",
        "\n",
        "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='step'):\n",
        "    #get initial Q values\n",
        "    Qcs = network.get_qs(env.getCS())\n",
        "\n",
        "    #get action from q values\n",
        "    action = np.argmax(Qcs,axis=1)-1\n",
        "    env.rm.alog += [(action==-1).sum(),(action==0).sum(),(action==1).sum()]\n",
        "\n",
        "    #epsilon randomization\n",
        "    r_action = randint(-1,2,BATCH_SIZE)\n",
        "    mask = np.random.rand(BATCH_SIZE)<epsilon\n",
        "\n",
        "    action[mask] = r_action[mask]\n",
        "    epsilon *= EPSILON_DECAY\n",
        "    epsilon = max(epsilon,MIN_EPSILON)\n",
        "\n",
        "    #run a step the batch\n",
        "    cs, ns, reward, done = env.step(action)\n",
        "    if episode%50 == 0:\n",
        "        env.rm.sslog.append(env.rm.slog)\n",
        "        env.rm.cclog.append(env.rm.clog)\n",
        "        env.rm.slog = 0\n",
        "        env.rm.clog = 0\n",
        "\n",
        "    if(episode%1000 == 0):\n",
        "          network.model.save('/content/drive/MyDrive/model2_alt')\n",
        "          print('Model saved - ' + str(epsilon))\n",
        "          episode += 1\n",
        "\n",
        "\n",
        "    #finally train the model\n",
        "    loss = network.train(cs, Qcs, ns, action, reward, done)\n",
        "    env.rm.llog.append(np.abs(loss))"
      ],
      "metadata": {
        "id": "JvsGyWBU_i_e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "cb074d8c-2863-4dc4-dab3-53356e43cf9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|1         | 999/75000 [02:18<2:24:02,  8.56step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/model2_alt/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|1         | 1000/75000 [02:19<9:06:02,  2.26step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved - 0.3529876077801247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|2         | 1999/75000 [04:24<2:32:44,  7.97step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/model2_alt/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|2         | 2000/75000 [04:25<7:03:08,  2.88step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved - 0.12979192838159928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|3         | 2999/75000 [06:32<2:31:53,  7.90step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/model2_alt/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|4         | 3000/75000 [06:33<7:08:54,  2.80step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved - 0.04772389823811472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|5         | 3999/75000 [08:38<2:23:34,  8.24step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/model2_alt/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|5         | 4000/75000 [08:39<7:58:55,  2.47step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved - 0.01754785903438985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|6         | 4999/75000 [10:45<2:24:27,  8.08step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/model2_alt/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|6         | 5000/75000 [10:46<6:47:11,  2.87step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved - 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|7         | 5999/75000 [12:50<2:36:46,  7.34step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/model2_alt/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|8         | 6000/75000 [12:51<7:02:34,  2.72step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved - 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|9         | 6999/75000 [14:56<2:39:22,  7.11step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/model2_alt/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|9         | 7000/75000 [14:57<6:59:00,  2.70step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved - 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|#         | 7999/75000 [17:02<2:20:44,  7.93step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/model2_alt/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|#         | 8000/75000 [17:03<6:29:31,  2.87step/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved - 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|#1        | 8695/75000 [18:31<2:21:15,  7.82step/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-95cdb2acd121>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPISODES\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#get initial Q values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mQcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetCS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#get action from q values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a9efe3e00b83>\u001b[0m in \u001b[0;36mget_qs\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m#return the q value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_qs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1976\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1978\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1979\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1980\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 755\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         self._resource_deleter = IteratorResourceDeleter(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3314\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3315\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 3316\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   3317\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3318\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the model"
      ],
      "metadata": {
        "id": "Mm-xgi_o6gm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network.model.save('/model1')"
      ],
      "metadata": {
        "id": "0nUqXf1X6kSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 10)"
      ],
      "metadata": {
        "id": "bhp3jdtZAkQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(env.rm.llog)),env.rm.llog)\n",
        "plt.xlabel(\"Loss over steps\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p9qqOrSiA5F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(env.rm.rlog)),env.rm.rlog)\n",
        "plt.xlabel(\"Reward over steps\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FBKiHL4Tne2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.rm.alog) #actions"
      ],
      "metadata": {
        "id": "Rr6hCA54BsEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(env.rm.cclog)),env.rm.cclog)\n",
        "plt.xlabel(\"Collision over steps\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1Iqn-D4oB2DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(env.rm.sslog)),env.rm.sslog)\n",
        "plt.xlabel(\"captures over steps\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jhUn_-GQB-fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(5116*256.0/env.rm.elog)"
      ],
      "metadata": {
        "id": "-o4MzvsE8Tcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(env.rm.sslog)"
      ],
      "metadata": {
        "id": "LlFO7aOeUUqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(env.rm.cclog)"
      ],
      "metadata": {
        "id": "hh0bSmDDUY-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dB_71GjHWeVb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}